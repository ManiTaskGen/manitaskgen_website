<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
     <title>ManiTaskGen: A Comprehensive Task Generator for
Benchmarking and Improving Vision-Language
Agents on Embodied Decision-Making</title>
    <link rel="stylesheet" href="custom.css">
    <link rel="icon" type="image/x-icon" href="./images/favicon.png">
</head>

<body>
    <h1 hidden></h1>

    <script src="https://kit.fontawesome.com/02e2ed10b8.js" crossorigin="anonymous"></script>

    <div>
        <div id="title-container">
            <p id="paper-title">ManiTaskGen: A Comprehensive Task Generator for
Benchmarking and Improving Vision-Language
Agents on Embodied Decision-Making</p>

            <div id="authors">
                <div>
                    <a class="author" href="https://liudai.notion.site/" target="_blank">Liu Dai<sup>1*</sup></a>
                    <a class="author" href="https://www.linkedin.com/in/haina-wang-58b91032b/" target="_blank">Haina Wang<sup>1*</sup></a>
                    <a class="author" href="https://www.ias.informatik.tu-darmstadt.de/Team/NicoBohlinger"
                        target="_blank">Weikang Wan<sup>1</sup></a>
                    <a class="author" href="https://cseweb.ucsd.edu/~haosu/" target="_blank">Hao Su<sup>1,2</sup></a>
                </div>
            </div>

            <div id="affiliations">
                <p class="affiliation"><sup>*</sup>Equal contribution</p>
                <p class="affiliation"><sup>1</sup>University of California San Diego, USA</p>
                <p class="affiliation"><sup>2</sup>Hillbot Inc, USA</p>
            </div>
        </div>

        <div id="social-buttons">
            <a href="https://arxiv.org/abs/2505.20726" target="_blank" class="social-button">
                <i class="fa fa-file-pdf"></i>
                <p>Paper</p>
            </a>
            <a href="https://youtu.be/62OBDgrQYw4" target="_blank" class="social-button">
                <i class="fa fa-youtube"></i>
                <p>[TALK (coming soon)]</p>
            </a>
        </div>

        <div id="tldr-container">
            <p id="tldr-text"><span id="tldr-first">TLDR:</span> We presents a universal task generation framework for arbitrary scenes, facilitating
both benchmarking and improvement of embodied decision-making agents. </p>
        </div>

        


        
        <div class="section-container">
            <p class="section-header">Demo</p>
            <p class="section-text">
                This work proposes universal system for generating comprehensive and diverse mobile manipulation tasks for arbitrary scenes.
            </p>
            <video class="demo-video" controls>
                <source src="videos/demo.mp4" type="video/mp4">
            </video>
        </div>

       <div class="section-container">
            <p class="section-header">Overview</p>
            
            <!-- 插入图片 -->
            <div class="section-image-container">
                <img src="images/ManiTaskGen_framework.png" alt="Task Generation Framework Overview" class="section-image">
            </div>

            <p class="section-text">
               ManiTaskGen is a universal system that generates a comprehensive set of feasible mobile manipulation tasks given arbitrary scene. These tasks facilitate automatic benchmarking and the improvement of embodied decision-making agents.
            </p>
        </div>

        

        <div class="section-container">
            <p class="section-header">Action space and Test flow</p>
            <p class="section-text">
                We define a discrete, abstracted action space that the VLM-based agent selects from at each timestep. The agent is equipped with abstracted navigation (go_to, change_view), grasping (pick) and placing (show_receptacle, place) skills.
            </p>
            <div class="section-deployment-video-container">
                <p class="section-deployment-video-container-header">Agents' Action Space</p>
                <div class="section-deployment-video-row">
                    <div class="deployment-video-block">
                        <p class="deployment-video-header">Navigate</p>
                        <img src="images/navigate_gif.gif" class="fixed-size-gif">
                    </div>
                    <div class="deployment-video-block">
                        <p class="deployment-video-header">Change View</p>
                        <img src="images/change_view_gif.gif"  class="fixed-size-gif">
                    </div>
                    
                </div>
                <div class="section-deployment-video-row">
                    <div class="deployment-video-block">
                        <p class="deployment-video-header">Show Receptacle</p>
                        <img src="images/show_receptacle_gif.gif" class="fixed-size-gif">
                    </div>
                    <div class="deployment-video-block">
                        <p class="deployment-video-header">Pick</p>
                        <img src="images/pick_gif.gif"  class="fixed-size-gif">
                    </div>
                    
                </div>
                <div class="section-deployment-video-row">
                    <div class="deployment-video-block">
                        <p class="deployment-video-header">Place R</p>
                        <img src="images/place_r_gif.gif" class="fixed-size-gif">
                    </div>
                    <div class="deployment-video-block">
                        <p class="deployment-video-header">Place S</p>
                        <img src="images/place_s_gif.gif" class="fixed-size-gif">
                    </div>
                </div>
                <!-- <p class="section-deployment-video-container-header">Test Flow</p> -->
            </div>
            <p class="section-text">
               At each timestep, the agent receives multimodal observations from the system, including rendered images (with tags for objects or, after "show_receptacle", for receptacles), and text information. The system executes the given actions, updates the environment, and provides the next observation. An episode terminates when the agent executes "CALL_END" or a preset timestep limit is reached. After termination, the benchmark automatically evaluates the episode, following the criteria illustrated in the next section.
            </p>
            <div class="section-image-container">
                <img src="images/testflow.png" alt="Task Generation Framework Overview" class="section-image">
            </div>

         
        </div>

        <div class="section-container">
            <p class="section-header">Experiments</p>
            <p class="section-sub-header">Task Difficulty and Evaluation Criteria</p>
            
            <div class="task-levels">

                <h3 class="subsection-header">Task Difficulty</h3>

                <p>We classify ManiTaskGen tasks into four difficulty levels, with increasing structual and perceptual complexity. The evaluation criteria for each task level are as follows:</p>
                
                <details class="expandable">
                    <summary>[expand]</summary>
                        <div class="expanded-text-content">

                            <p class="level-item"><strong>Level 1 tasks:</strong> Single-step pick-and-place tasks involving unique target objects.</p>
                    
                            <p class="level-item"><strong>Level 2 tasks:</strong> Single-step pick-and-place tasks involving non-unique target objects, which requires disambiguation.</p>
                            
                            <p class="level-item"><strong>Level 3 tasks:</strong> Multi-step pick-and-place tasks involving non-unique target objects, formed by chaining level 1 or 2 tasks with logical connector THEN.</p>
                            
                            <p class="level-item"><strong>Level 4 tasks:</strong> Outcome-based tasks, which require the agent to achieve a specific arrangement or configuration of objects, such as cascading or stacking.</p>
                    
                        </div>
                </details>

                <h3 class="subsection-header">Evaluation Criteria</h3>

                <p>For each testing episode of process-based tasks, we conclude following evaluation metrics:</p>
                

                <details class="expandable">
                    <summary>[expand]</summary>
                        <div class="expanded-text-content">

                        <p class="metric-item"><strong>(1) Success Rate (SR):</strong> Whether the task was completed successfully.</p>
                
                        <p class="metric-item"><strong>(2) Intermediate Points (IP):</strong> A fine-grained measurement of task completion progress.</p>
                        
                        <p>For Level 1 & 2 tasks, a successful episode should include the following four sub-steps, each contributing 25 points of IP:</p>
                        
                        <ul class="ip-steps">
                            <li>Navigate to the correct starting location</li>
                            <li>Grasp the correct object</li>
                            <li>Navigate to the correct destination with the right object</li>
                            <li>Place the right object in the correct place</li>
                        </ul>
                        
                        <p>IP for Level 3 tasks is computed by averaging points from the two sequential sub-tasks.</p>
                        
                    </div>
                </details>

                
                
                <p> Benchmarking Level 4 tasks is left for future work.</p>
           
            </div>
                
                
                
                
          

           <div class="section-image-container">
                <img src="images/task_sample.png" alt="Examples of different level tasks" class="section-image">
            </div>

        <div class="demo-container">
            <div class="task-panel">
                <h3>Examples</h3>
                <div class="task-item selected">
                <p>Move kitchenware_white_paper_towel_holder between lighting_blue_base_table_lamp and lighting_blue_base_table_lamp</p>
                <p class="task-description">(Success)</p>
                </div>
            </div>
            
            <div class="chat-interface">
                <div class="control-panel">
                <div class="control-group">
                    <label>VLM:</label>
                    <select class="control-select">
                    <option selected>gpt-4o-mini</option>
                    </select>
                </div>
                
                <div class="control-group">
                    <label>Query Mode:</label>
                    <select class="control-select">
                    <option selected>Subgoals</option>
                    </select>
                </div>
                
                <div class="control-group">
                    <label>Robot:</label>
                    <select class="control-select">
                    <option selected>Dual-Arm PR2</option>
                    </select>
                </div>
                
                <div class="control-group">
                    <label>Init:</label>
                    <select class="control-select">
                    <option selected>Open Spaces</option>
                    </select>
                </div>
                </div>
                
                <div class="chat-content">
                <div class="chat-message robot-message">
                    <div class="message-image">
                    <img src="images/steps/success_episode_picture/step0.PNG" alt="Kitchen scene with robot">
                    </div>
                    <div class="message-text">
                    
                    <p>You are a household assistant robot equipped with a vision system and manipulation capabilities. You need to complete a task in a 3D environment, mainly involving moving, picking up, and placing objects. Your goal is to move an object from one place to another correctly according to instructions. Please strictly follow the given Action Space for operations.</p>
                    
                    <p>At the beginning of each task, you will be informed of a specific task to complete. Throughout the task execution, you will transition between multiple states and have access to a set of possible actions (action space).</p>

                    <details class="expandable">
                        <summary>[expand]</summary>
                        <div class="expanded-content">
                        <p>['chicken leg', 'cabinet right door', 'cabinet left door', 'cabinet', 'stove on the left', 'stove on the right', 'counter top on the left', 'counter top on the right', 'pot lid', 'pot body', 'pot bottom', 'stove knob on the right', 'stove knob on the left', 'kitchen sink', 'faucet handle', 'fridge', 'fridge door', 'fridge shelf', 'salt shaker', 'pepper shaker'].</p>
                        
                        <p>Currently, you can see the following objects:<br>
                        "the pot lid is on the stove on the left,<br>
                        the pot body is on the counter top on the right,<br>
                        the chicken leg is on the fridge shelf,<br>
                        the salt shaker is in the cabinet,<br>
                        the pepper shaker is in the cabinet,<br>
                        cabinet right door is partially open,<br>
                        cabinet left door is partially open,<br>
                        fridge door is partially open,<br>
                        stove knob on the right is turned off,<br>
                        stove knob on the left is turned off,<br>
                        faucet handle is turned off"</p>
                        </div>
                    </details>
                    
                    <p>You have already taken the following actions written in a formal language:</p>
                    <ol>
                        <li>['picked', 'chicken leg']</li>
                        <li>['closedjoint', 'fridge door']</li>
                        <li>['picked', 'salt shaker']</li>
                        <li>['closedjoint', 'cabinet left door']</li>
                        <li>['picked', 'pepper shaker']</li>
                        <li>['closedjoint', 'cabinet right door']</li>
                        <li>['sprinkledto', 'salt shaker', 'pot body']</li>
                        <li>['sprinkledto', 'pepper shaker', 'pot body']</li>
                    </ol>
                    
                    <p>Currently, the robot is holding some objects. The right hand is holding pepper shaker. The left hand is holding salt shaker.</p>
                    
                    <p>You just failed at planning for subgoals ['holding', 'pot lid']. So please do not list this subgoals as the first subgoals to achieve in your answer.</p>
                    
                    <p>You are a mobile robot with two arms. You must obey the following commonsense rules:</p>
                    
                    <details class="expandable">
                        <summary>[expand]</summary>
                        <div class="expanded-content">
                        <ol>
                            <li>You must have at least one empty hand before you can pick up an object or open or close a joint.</li>
                            <li>When you sprinkle or pour something into a container, there must not be objects placed on top of the container.</li>
                            <li>You can only take actions on objects that you can see.</li>
                            <li>If you cannot see an object, it may be behind a door or inside drawer.</li>
                            <li>If you cannot see the inside of a space, you must open its door or drawer before you can pick objects from it or place objects inside it.</li>
                        </ol>
                        </div>
                    </details>
                    </div>
                </div>
                
                <div class="chat-message human-message">
                    <div class="message-text">
                    <ol>
                        <li>Open the cabinet left door.</li>
                        <li>Pick up the pot lid from the cabinet.</li>
                        <li>Close the cabinet left door.</li>
                        <li>Place the pot lid on the pot body on the counter top on the right.</li>
                        <li>Turn on the stove knob on the left.</li>
                        <li>Wait for the chicken leg in the pot body to cook.</li>
                    </ol>
                    </div>
                </div>
                </div>
            </div>
            </div>

        
            <div class="section-container">
            <p class="section-header">Results</p>
            <p class="section-text">
                We observe that all models achieve a low average SR (under 30%), significantly below human-level performance. Performance generally decreases with increasing task difficulty. Longer tasks (Level 3) lead to significantly lower performance compared to single-step tasks, highlighting the substantial challenge posed by the generated tasks.
            </p>
            <img src="images/result_table.png" alt="result_table" class="section-image">
            <img src="images/sankey_diagram.png" alt="result_table" class="section-image">
        </div>

        <div class="section-container">
            <p class="section-header">Acknowledgments</p>
            <p class="section-text">
            </p>
        </div>

        
            <p class="section-header">Citation</p>
            <div class="citation-content">
                <pre>@misc{dai2025manitaskgencomprehensivetaskgenerator,
                    title={ManiTaskGen: A Comprehensive Task Generator for Benchmarking and Improving Vision-Language Agents on Embodied Decision-Making}, 
                    author={Liu Dai and Haina Wang and Weikang Wan and Hao Su},
                    year={2025},
                    eprint={2505.20726},
                    archivePrefix={arXiv},
                    primaryClass={cs.RO},
                    url={https://arxiv.org/abs/2505.20726}, 
                }</pre>
            </div>
       

        <div id="website-inspiration">
            <p>This website was inspired by <a href="https://kzakka.com/robopianist/">Kevin Zakka's</a> and <a
                    href="https://brentyi.github.io/tilted/">Brent Yi's</a> and <a href="embodiment-scaling-laws.github.io/">  </a> Nico Bohlinger's</a>.
            </p>
        </div>
    </div>
</body>

</html> 
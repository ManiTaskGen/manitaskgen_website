<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
     <title>ManiTaskGen: A Comprehensive Task Generator for
Benchmarking and Improving Vision-Language
Agents on Embodied Decision-Making</title>
    <link rel="stylesheet" href="custom.css">
    <link rel="icon" type="image/x-icon" href="./images/favicon.png">
</head>

<body>
    <h1 hidden></h1>

    <script src="https://kit.fontawesome.com/02e2ed10b8.js" crossorigin="anonymous"></script>

    <div>
        <div id="title-container">
            <p id="paper-title">ManiTaskGen: A Comprehensive Task Generator for
Benchmarking and Improving Vision-Language
Agents on Embodied Decision-Making</p>

            <div id="authors">
                <div>
                    <a class="author" href="https://liudai.notion.site/" target="_blank">Liu Dai<sup>1*</sup></a>
                    <a class="author" href="https://www.linkedin.com/in/haina-wang-58b91032b/" target="_blank">Haina Wang<sup>1*</sup></a>
                    <a class="author" href="https://www.ias.informatik.tu-darmstadt.de/Team/NicoBohlinger"
                        target="_blank">Weikang Wan<sup>1</sup></a>
                    <a class="author" href="https://cseweb.ucsd.edu/~haosu/" target="_blank">Hao Su<sup>1,2</sup></a>
                </div>
            </div>

            <div id="affiliations">
                <p class="affiliation"><sup>*</sup>Equal contribution</p>
                <p class="affiliation"><sup>1</sup>University of California San Diego, USA</p>
                <p class="affiliation"><sup>2</sup>Hillbot Inc, USA</p>
            </div>
        </div>

        <div id="social-buttons">
            <a href="https://arxiv.org/abs/2505.20726" target="_blank" class="social-button">
                <i class="fa fa-file-pdf"></i>
                <p>Paper</p>
            </a>
            <a href="https://youtu.be/62OBDgrQYw4" target="_blank" class="social-button">
                <i class="fa fa-youtube"></i>
                <p>[TALK (coming soon)]</p>
            </a>
        </div>

        <div id="tldr-container">
            <p id="tldr-text"><span id="tldr-first">TLDR:</span> We presents a universal task generation framework for arbitrary scenes, facilitating
both benchmarking and improvement of embodied decision-making agents. </p>
        </div>

        
        <div class="section-container">
            <p class="section-header">Overview</p>
            
            <!-- ÊèíÂÖ•ÂõæÁâá -->
            <div class="section-image-container">
                <img src="images/ManiTaskGen_framework.png" alt="Task Generation Framework Overview" class="section-image">
            </div>

            <p class="section-text">
               ManiTaskGen is a universal system that generates a comprehensive set of feasible mobile manipulation tasks given arbitrary scene. These tasks facilitate automatic benchmarking and the improvement of embodied decision-making agents.
            </p>
        </div>

        
        <div class="section-container">
            <p class="section-header" style="text-align: center;">Comprehensive Task Generation </p>
            <p class="section-text">
                The first part of this project is a new methodology for generating comprehensive sets of process- and outcome-based tasks.
            </p>
             <video class="demo-video" controls>
                <source src="videos/task_gen.mp4" type="video/mp4">
            </video>
            
        </div>

        <div class="section-container">
            <p class="section-sub-header" style="text-align: center;">Receptacle-Aware 3D Scene Graph Construction</p>
            <p class="section-text">
                We build a scene graph ùíÆ that captures the spatial and functional relationships between objects and their receptacles. To support retrieving multiple refined receptacles, the merging-receptacle technique is also applied.
                This method spells the generated tasks covering more objects and locations.
            </p>
            <div class="section-image-container">
                <img src="images/receptacles.png" alt="Task Generation Framework Overview" class="section-image">
                <img src="images/heatmap.png" alt="Task Generation Framework Overview" class="section-image">
            </div>
        </div>

        <div class="section-container">
            <p class="section-sub-header" style="text-align: center;">Generating Tasks</p>
            <p class="section-text">
                For process-based tasks, we generate them through the use of our scene graph and some patterns.
                For outcome-based tasks, we employ a hybrid approach combining template-based generation and VLM-based filtering.
            </p>
            <div class="section-image-container">
                <img src="images/atomic_tasks.png" alt="Task Generation Framework Overview" class="section-image">
                <img src="images/outcome_based_tasks.png" alt="Task Generation Framework Overview" class="section-image">
            </div>
        </div>

        

        <div class="section-container">
            <p class="section-header">Benchmarking VLM Agents</p>

            The second part of this project is benchmarking VLM agents using the generated tasks.
            <video class="demo-video" controls>
                <source src="videos/demo.mp4" type="video/mp4">
            </video>
        </div>

       

        

        <div class="section-container">
           <p class="section-sub-header" style="text-align: center;">Action space and Test flow</p>
            <p class="section-text">
                We define a discrete, abstracted action space that the VLM-based agent selects from at each timestep. The agent is equipped with abstracted navigation (go_to, change_view), grasping (pick) and placing (show_receptacle, place) skills.
            </p>

            <div class="section-deployment-video-container">
            <p class="section-deployment-video-container-header">Agents' Action Space</p>
                
                <div class="section-deployment-video-row">
                    <div class="deployment-video-block">
                        <p class="deployment-video-header">Goto</p>
                        <div class="video-container">
                            <video class="control-video">
                                <source src="videos/goto_mp4.mp4" type="video/mp4">
                                Your browser does not support HTML5 video
                            </video>
                            <div class="video-controls">
                                <button class="play-button">play</button>
                                <div class="speed-control">
                                    <label>speed:</label>
                                    <select class="speed-selector">
                                        <option value="0.5">0.5√ó</option>
                                        <option value="0.75">0.75√ó</option>
                                        <option value="1" selected>1.0√ó</option>
                                        <option value="1.5">1.5√ó</option>
                                    </select>
                                </div>
                            </div>
                        </div>
                    </div>
                    
                    <div class="deployment-video-block">
                        <p class="deployment-video-header">Change View</p>
                        <div class="video-container">
                            <video class="control-video">
                                <source src="videos/change_view_mp4.mp4" type="video/mp4">
                                Your browser does not support HTML5 video
                            </video>
                            <div class="video-controls">
                                <button class="play-button">Play</button>
                                <div class="speed-control">
                                    <label>Speed:</label>
                                    <select class="speed-selector">
                                        <option value="0.5">0.5√ó</option>
                                        <option value="0.75">0.75√ó</option>
                                        <option value="1" selected>1.0√ó</option>
                                        <option value="1.5">1.5√ó</option>
                                    </select>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="section-deployment-video-row">
                    <div class="deployment-video-block">
                        <p class="deployment-video-header">Pick</p>
                        <div class="video-container">
                            <video class="control-video">
                                <source src="videos/pick_mp4.mp4" type="video/mp4">
                                Your browser does not support HTML5 video
                            </video>
                            <div class="video-controls">
                                <button class="play-button">play</button>
                                <div class="speed-control">
                                    <label>speed:</label>
                                    <select class="speed-selector">
                                        <option value="0.5">0.5√ó</option>
                                        <option value="0.75">0.75√ó</option>
                                        <option value="1" selected>1.0√ó</option>
                                        <option value="1.5">1.5√ó</option>
                                    </select>
                                </div>
                            </div>
                        </div>
                    </div>
                    
                    <div class="deployment-video-block">
                        <p class="deployment-video-header">Show Receptacle</p>
                        <div class="video-container">
                            <video class="control-video">
                                <source src="videos/show_receptacle_mp4.mp4" type="video/mp4">
                                Your browser does not support HTML5 video
                            </video>
                            <div class="video-controls">
                                <button class="play-button">Play</button>
                                <div class="speed-control">
                                    <label>Speed:</label>
                                    <select class="speed-selector">
                                        <option value="0.5">0.5√ó</option>
                                        <option value="0.75">0.75√ó</option>
                                        <option value="1" selected>1.0√ó</option>
                                        <option value="1.5">1.5√ó</option>
                                    </select>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>

                <div class="section-deployment-video-row">
                    <div class="deployment-video-block">
                        <p class="deployment-video-header">Place_r</p>
                        <div class="video-container">
                            <video class="control-video">
                                <source src="videos/place_r_mp4.mp4" type="video/mp4">
                                Your browser does not support HTML5 video
                            </video>
                            <div class="video-controls">
                                <button class="play-button">play</button>
                                <div class="speed-control">
                                    <label>speed:</label>
                                    <select class="speed-selector">
                                        <option value="0.5">0.5√ó</option>
                                        <option value="0.75">0.75√ó</option>
                                        <option value="1" selected>1.0√ó</option>
                                        <option value="1.5">1.5√ó</option>
                                    </select>
                                </div>
                            </div>
                        </div>
                    </div>
                    
                    <div class="deployment-video-block">
                        <p class="deployment-video-header">Place_s</p>
                        <div class="video-container">
                            <video class="control-video">
                                <source src="videos/place_s_mp4.mp4" type="video/mp4">
                                Your browser does not support HTML5 video
                            </video>
                            <div class="video-controls">
                                <button class="play-button">Play</button>
                                <div class="speed-control">
                                    <label>Speed:</label>
                                    <select class="speed-selector">
                                        <option value="0.5">0.5√ó</option>
                                        <option value="0.75">0.75√ó</option>
                                        <option value="1" selected>1.0√ó</option>
                                        <option value="1.5">1.5√ó</option>
                                    </select>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
               


            </div>

            <p class="section-text">
               At each timestep, the agent receives multimodal observations from the system, including rendered images (with tags for objects or, after "show_receptacle", for receptacles), and text information. The system executes the given actions, updates the environment, and provides the next observation. An episode terminates when the agent executes "CALL_END" or a preset timestep limit is reached. After termination, the benchmark automatically evaluates the episode, following the criteria illustrated in the next section.
            </p>
            <div class="section-image-container">
                <img src="images/testflow.png" alt="Task Generation Framework Overview" class="section-image">
            </div>


            <p class="section-sub-header" style="text-align: center;">Task Difficulty and Evaluation Criteria</p>
            
            <div class="section-image-container">
                <img src="images/task_judgement.png" alt="task_judgement_method" class="section-image">
            </div>


            <div class="task-levels" sylte="text-align: left;">
                              
                <p> Benchmarking Level 4 tasks and its Evaluation criteria is left for future work.</p>
                <p> Some Real Episode is displayed below: </p>
           
            </div>
                
                
                
        

        
        <div class="demo-container">
            <div class="task-panel">
                <h3>Examples</h3>
                <div class="task-item selected">
                <p>Move kitchenware_white_paper_towel_holder between lighting_blue_base_table_lamp and lighting_blue_base_table_lamp</p>
                <p class="task-description">(Success)</p>
                </div>
            </div>
            
            <div class="chat-interface">
                <div class="control-panel">
                <div class="control-group">
                    <label>VLM:</label>
                    <select class="control-select">
                    <option selected>gpt-4o-mini</option>
                    </select>
                </div>
                
                <div class="control-group">
                    <label>Query Mode:</label>
                    <select class="control-select">
                    <option selected>Subgoals</option>
                    </select>
                </div>
                
                <div class="control-group">
                    <label>Robot:</label>
                    <select class="control-select">
                    <option selected>Dual-Arm PR2</option>
                    </select>
                </div>
                
                <div class="control-group">
                    <label>Init:</label>
                    <select class="control-select">
                    <option selected>Open Spaces</option>
                    </select>
                </div>
                </div>
                
                <div class="chat-content">
                <div class="chat-message robot-message">
                    <div class="message-image">
                    <img src="images/steps/success_episode_picture/step0.PNG" alt="Kitchen scene with robot">
                    </div>
                    <div class="message-text">
                    
                    <p>You are a household assistant robot equipped with a vision system and manipulation capabilities. You need to complete a task in a 3D environment, mainly involving moving, picking up, and placing objects. Your goal is to move an object from one place to another correctly according to instructions. Please strictly follow the given Action Space for operations.</p>
                    
                    <p>At the beginning of each task, you will be informed of a specific task to complete. Throughout the task execution, you will transition between multiple states and have access to a set of possible actions (action space).</p>

                    <details class="expandable">
                        <summary>[expand]</summary>
                        <div class="expanded-content">
                        <p>['chicken leg', 'cabinet right door', 'cabinet left door', 'cabinet', 'stove on the left', 'stove on the right', 'counter top on the left', 'counter top on the right', 'pot lid', 'pot body', 'pot bottom', 'stove knob on the right', 'stove knob on the left', 'kitchen sink', 'faucet handle', 'fridge', 'fridge door', 'fridge shelf', 'salt shaker', 'pepper shaker'].</p>
                        
                        <p>Currently, you can see the following objects:<br>
                        "the pot lid is on the stove on the left,<br>
                        the pot body is on the counter top on the right,<br>
                        the chicken leg is on the fridge shelf,<br>
                        the salt shaker is in the cabinet,<br>
                        the pepper shaker is in the cabinet,<br>
                        cabinet right door is partially open,<br>
                        cabinet left door is partially open,<br>
                        fridge door is partially open,<br>
                        stove knob on the right is turned off,<br>
                        stove knob on the left is turned off,<br>
                        faucet handle is turned off"</p>
                        </div>
                    </details>
                    
                    <p>You have already taken the following actions written in a formal language:</p>
                    <ol>
                        <li>['picked', 'chicken leg']</li>
                        <li>['closedjoint', 'fridge door']</li>
                        <li>['picked', 'salt shaker']</li>
                        <li>['closedjoint', 'cabinet left door']</li>
                        <li>['picked', 'pepper shaker']</li>
                        <li>['closedjoint', 'cabinet right door']</li>
                        <li>['sprinkledto', 'salt shaker', 'pot body']</li>
                        <li>['sprinkledto', 'pepper shaker', 'pot body']</li>
                    </ol>
                    
                    <p>Currently, the robot is holding some objects. The right hand is holding pepper shaker. The left hand is holding salt shaker.</p>
                    
                    <p>You just failed at planning for subgoals ['holding', 'pot lid']. So please do not list this subgoals as the first subgoals to achieve in your answer.</p>
                    
                    <p>You are a mobile robot with two arms. You must obey the following commonsense rules:</p>
                    
                    <details class="expandable">
                        <summary>[expand]</summary>
                        <div class="expanded-content">
                        <ol>
                            <li>You must have at least one empty hand before you can pick up an object or open or close a joint.</li>
                            <li>When you sprinkle or pour something into a container, there must not be objects placed on top of the container.</li>
                            <li>You can only take actions on objects that you can see.</li>
                            <li>If you cannot see an object, it may be behind a door or inside drawer.</li>
                            <li>If you cannot see the inside of a space, you must open its door or drawer before you can pick objects from it or place objects inside it.</li>
                        </ol>
                        </div>
                    </details>
                    </div>
                </div>
                
                <div class="chat-message human-message">
                    <div class="message-text">
                    <ol>
                        <li>Open the cabinet left door.</li>
                        <li>Pick up the pot lid from the cabinet.</li>
                        <li>Close the cabinet left door.</li>
                        <li>Place the pot lid on the pot body on the counter top on the right.</li>
                        <li>Turn on the stove knob on the left.</li>
                        <li>Wait for the chicken leg in the pot body to cook.</li>
                    </ol>
                    </div>
                </div>
                </div>
            </div>
            </div>

            <p class="section-sub-header" style="text-align: center;">Benchmarking Results</p>
            

            
        
            <div class="section-container">
               
                <p class="section-text">
                    We observe that all models achieve a low average SR (under 30%), significantly below human-level performance. Performance generally decreases with increasing task difficulty. Longer tasks (Level 3) lead to significantly lower performance compared to single-step tasks, highlighting the substantial challenge posed by the generated tasks.
                    
                    Specifically, we design a self-reflection model that processes the evaluation results of each episode to generate a verbal summary. This summary is then stored in a long-term memory and used as part of the input for the agent when attempting the task in future episodes
                    
                </p>
                <img src="images/result_table.png" alt="result_table" class="section-image">
                <img src="images/sankey_diagram.png" alt="result_table" class="section-image">
            </div>
        </div>

            

        <div class="section-container">
            <p class="section-header">Reflection</p>

            ManiTaskGen tasks can not only be used for benchmarking VLMs, but also for optimizing VLM-based agents. we adopt an inference-time RFT policy to enhance agent capabilities inspired by Reflexion and ReAct.

            
        <div class="demo-container">
            <div class="task-panel">
                <h3>Examples</h3>
                <div class="task-item selected">
                <p>Move kitchenware_white_paper_towel_holder between lighting_blue_base_table_lamp and lighting_blue_base_table_lamp</p>
                <p class="task-description">(Success)</p>
                </div>
            </div>
            
            <div class="chat-interface">
                <div class="control-panel">
                <div class="control-group">
                    <label>VLM:</label>
                    <select class="control-select">
                    <option selected>gpt-4o-mini</option>
                    </select>
                </div>
                
                <div class="control-group">
                    <label>Query Mode:</label>
                    <select class="control-select">
                    <option selected>Subgoals</option>
                    </select>
                </div>
                
                <div class="control-group">
                    <label>Robot:</label>
                    <select class="control-select">
                    <option selected>Dual-Arm PR2</option>
                    </select>
                </div>
                
                <div class="control-group">
                    <label>Init:</label>
                    <select class="control-select">
                    <option selected>Open Spaces</option>
                    </select>
                </div>
                </div>
                
                <div class="chat-content">
                <div class="chat-message robot-message">
                    <div class="message-image">
                    <img src="images/steps/success_episode_picture/step0.PNG" alt="Kitchen scene with robot">
                    </div>
                    <div class="message-text">
                    
                    <p>You are a household assistant robot equipped with a vision system and manipulation capabilities. You need to complete a task in a 3D environment, mainly involving moving, picking up, and placing objects. Your goal is to move an object from one place to another correctly according to instructions. Please strictly follow the given Action Space for operations.</p>
                    
                    <p>At the beginning of each task, you will be informed of a specific task to complete. Throughout the task execution, you will transition between multiple states and have access to a set of possible actions (action space).</p>

                    <details class="expandable">
                        <summary>[expand]</summary>
                        <div class="expanded-content">
                        <p>['chicken leg', 'cabinet right door', 'cabinet left door', 'cabinet', 'stove on the left', 'stove on the right', 'counter top on the left', 'counter top on the right', 'pot lid', 'pot body', 'pot bottom', 'stove knob on the right', 'stove knob on the left', 'kitchen sink', 'faucet handle', 'fridge', 'fridge door', 'fridge shelf', 'salt shaker', 'pepper shaker'].</p>
                        
                        <p>Currently, you can see the following objects:<br>
                        "the pot lid is on the stove on the left,<br>
                        the pot body is on the counter top on the right,<br>
                        the chicken leg is on the fridge shelf,<br>
                        the salt shaker is in the cabinet,<br>
                        the pepper shaker is in the cabinet,<br>
                        cabinet right door is partially open,<br>
                        cabinet left door is partially open,<br>
                        fridge door is partially open,<br>
                        stove knob on the right is turned off,<br>
                        stove knob on the left is turned off,<br>
                        faucet handle is turned off"</p>
                        </div>
                    </details>
                    
                    <p>You have already taken the following actions written in a formal language:</p>
                    <ol>
                        <li>['picked', 'chicken leg']</li>
                        <li>['closedjoint', 'fridge door']</li>
                        <li>['picked', 'salt shaker']</li>
                        <li>['closedjoint', 'cabinet left door']</li>
                        <li>['picked', 'pepper shaker']</li>
                        <li>['closedjoint', 'cabinet right door']</li>
                        <li>['sprinkledto', 'salt shaker', 'pot body']</li>
                        <li>['sprinkledto', 'pepper shaker', 'pot body']</li>
                    </ol>
                    
                    <p>Currently, the robot is holding some objects. The right hand is holding pepper shaker. The left hand is holding salt shaker.</p>
                    
                    <p>You just failed at planning for subgoals ['holding', 'pot lid']. So please do not list this subgoals as the first subgoals to achieve in your answer.</p>
                    
                    <p>You are a mobile robot with two arms. You must obey the following commonsense rules:</p>
                    
                    <details class="expandable">
                        <summary>[expand]</summary>
                        <div class="expanded-content">
                        <ol>
                            <li>You must have at least one empty hand before you can pick up an object or open or close a joint.</li>
                            <li>When you sprinkle or pour something into a container, there must not be objects placed on top of the container.</li>
                            <li>You can only take actions on objects that you can see.</li>
                            <li>If you cannot see an object, it may be behind a door or inside drawer.</li>
                            <li>If you cannot see the inside of a space, you must open its door or drawer before you can pick objects from it or place objects inside it.</li>
                        </ol>
                        </div>
                    </details>
                    </div>
                </div>
                
                <div class="chat-message human-message">
                    <div class="message-text">
                    <ol>
                        <li>Open the cabinet left door.</li>
                        <li>Pick up the pot lid from the cabinet.</li>
                        <li>Close the cabinet left door.</li>
                        <li>Place the pot lid on the pot body on the counter top on the right.</li>
                        <li>Turn on the stove knob on the left.</li>
                        <li>Wait for the chicken leg in the pot body to cook.</li>
                    </ol>
                    </div>
                </div>
                </div>
            </div>
            </div>


            <p class="section-text">
            </p>
        </div>

        <div class="section-container">
            <p class="section-header">Acknowledgments</p>
            <p class="section-text">
            </p>
        </div>

        
            <p class="section-header">Citation</p>
            <div class="citation-content">
                <pre>@misc{dai2025manitaskgencomprehensivetaskgenerator,
                    title={ManiTaskGen: A Comprehensive Task Generator for Benchmarking and Improving Vision-Language Agents on Embodied Decision-Making}, 
                    author={Liu Dai and Haina Wang and Weikang Wan and Hao Su},
                    year={2025},
                    eprint={2505.20726},
                    archivePrefix={arXiv},
                    primaryClass={cs.RO},
                    url={https://arxiv.org/abs/2505.20726}, 
                }</pre>
            </div>
       

        <div id="website-inspiration">
            <p>This website was inspired by <a href="https://kzakka.com/robopianist/">Kevin Zakka's</a> and <a
                    href="https://brentyi.github.io/tilted/">Brent Yi's</a> and <a href="embodiment-scaling-laws.github.io/">  </a> Nico Bohlinger's</a>.
            </p>
        </div>
    </div>
</body>

<script>
document.addEventListener('DOMContentLoaded', function() {
    // Ëé∑ÂèñÊâÄÊúâËßÜÈ¢ëÂÖÉÁ¥†
    const videos = document.querySelectorAll('.control-video');
    
    videos.forEach(video => {
        const container = video.closest('.video-container');
        const playButton = container.querySelector('.play-button');
        const speedSelector = container.querySelector('.speed-selector');
        
        // Êí≠Êîæ/ÊöÇÂÅúÂäüËÉΩ
        playButton.addEventListener('click', function() {
            if (video.paused || video.ended) {
                video.play();
                playButton.textContent = 'Pause';
            } else {
                video.pause();
                playButton.textContent = 'Play';
            }
        });
        
        // ÈÄüÂ∫¶ÊéßÂà∂ÂäüËÉΩ
        speedSelector.addEventListener('change', function() {
            video.playbackRate = parseFloat(this.value);
        });
        
        // ËßÜÈ¢ëÁªìÊùüÂêéÈáçÁΩÆÊåâÈíÆ
        video.addEventListener('ended', function() {
            playButton.textContent = 'Play';
        });
        
        // ÁÇπÂáªËßÜÈ¢ëÊú¨Ë∫´‰πüËÉΩÊí≠Êîæ/ÊöÇÂÅú
        video.addEventListener('click', function() {
            if (video.paused || video.ended) {
                video.play();
                playButton.textContent = 'Pause';
            } else {
                video.pause();
                playButton.textContent = 'Play';
            }
        });
    });
});
</script>

</html> 